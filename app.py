import streamlit as st
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from langchain import LLMChain, PromptTemplate
from langchain_groq import ChatGroq
import os
import re
import json
from dotenv import load_dotenv
import matplotlib.pyplot as plt

load_dotenv()

# Configurar el modelo LLM
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")
llm = ChatGroq(
    model="gemma2-9b-it",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

noticias = [
    "Repsol, entre las 50 empresas que m√°s responsabilidad hist√≥rica tienen en el calentamiento global",
    "Amancio Ortega crea un fondo de 100 millones de euros para los afectados de la dana",
    "Freshly Cosmetics despide a 52 empleados en Reus, el 18% de la plantilla",
    "Wall Street y los mercados globales caen ante la incertidumbre por la guerra comercial y el temor a una recesi√≥n",
    "El mercado de criptomonedas se desploma: Bitcoin cae a 80.000 d√≥lares, las altcoins se hunden en medio de una fren√©tica liquidaci√≥n",
    "Granada retrasa seis meses el inicio de la Zona de Bajas Emisiones, previsto hasta ahora para abril",
    "McDonald's donar√° a la Fundaci√≥n Ronald McDonald todas las ganancias por ventas del Big Mac del 6 de diciembre",
    "El Gobierno autoriza a altos cargos p√∫blicos a irse a Indra, Escribano, CEOE, Barcel√≥, Iberdrola o Airbus",
    "Las aportaciones a los planes de pensiones caen 10.000 millones en los √∫ltimos cuatro a√±os",
]
plantilla_reaccion = """
Reacci√≥n del inversor: {reaccion}
Analiza el sentimiento y la preocupaci√≥n expresada.  
Clasifica la preocupaci√≥n principal en una de estas categor√≠as:  
- Ambiental  
- Social  
- Gobernanza  
- Riesgo  

Si la respuesta es demasiado breve o poco clara, devuelve "INSUFICIENTE".

Luego, genera una pregunta de seguimiento enfocada en la categor√≠a detectada para profundizar en la opini√≥n del inversor.
"""
prompt_reaccion = PromptTemplate(template=plantilla_reaccion, input_variables=["reaccion"])
cadena_reaccion = LLMChain(llm=llm, prompt=prompt_reaccion)

if "historial" not in st.session_state:
    st.session_state.historial = []
    st.session_state.contador = 0
    st.session_state.mostrada_noticia = False

st.title("Chatbot de An√°lisis de Sentimiento")

for mensaje in st.session_state.historial:
    with st.chat_message(mensaje["tipo"]):
        st.write(mensaje["contenido"])

if st.session_state.contador < len(noticias):
    if not st.session_state.mostrada_noticia:
        noticia = noticias[st.session_state.contador]
        with st.chat_message("bot", avatar="ü§ñ"):
            st.write(f"¬øQu√© opinas sobre esta noticia? {noticia}")
        st.session_state.historial.append({"tipo": "bot", "contenido": noticia})
        st.session_state.mostrada_noticia = True

    user_input = st.chat_input("Escribe tu respuesta aqu√≠...")
    if user_input:
        st.session_state.historial.append({"tipo": "user", "contenido": user_input})
        analisis_reaccion = cadena_reaccion.run(reaccion=user_input)
        
        if "INSUFICIENTE" in analisis_reaccion:
            with st.chat_message("bot", avatar="ü§ñ"):
                st.write("Tu respuesta es muy breve o poco clara. ¬øPodr√≠as ampliarla?")
            st.session_state.historial.append({"tipo": "bot", "contenido": "Tu respuesta es muy breve o poco clara. ¬øPodr√≠as ampliarla?"})
        else:
            with st.chat_message("bot", avatar="ü§ñ"):
                st.write(analisis_reaccion)
            st.session_state.historial.append({"tipo": "bot", "contenido": analisis_reaccion})
            st.session_state.contador += 1
            st.session_state.mostrada_noticia = False
            st.rerun()
else:
    st.write("An√°lisis completado. Gracias por participar.")
